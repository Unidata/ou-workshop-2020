{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded daily summary data from the Oklahoma Mesonet for the Norman sites from 1994 to now.  Our first step is to load this data in using pandas (which you learned about this morning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import our libraries.  Run this cell once to bring in all the correct libraries\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('NormanMesonet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['YEAR', 'MONTH', 'DAY', 'STID', 'TMAX', 'TMIN', 'TAVG', 'DMAX', 'DMIN',\n",
      "       'DAVG', 'HMAX', 'HMIN', 'HAVG', 'VDEF', '9AVG', 'HDEG', 'CDEG', 'HTMX',\n",
      "       'WCMN', 'PDIR', 'PDFQ', 'SDIR', 'SDFQ', 'WSMX', 'WSMN', 'WSPD', 'WDEV',\n",
      "       'WMAX', '2MAX', '2MIN', '2AVG', '2DEV', 'PMAX', 'PMIN', 'PAVG', 'MSLP',\n",
      "       'AMAX', 'ATOT', 'RAIN', 'RNUM', 'RMAX', 'SMAX', 'SMIN', 'SAVG', 'BMAX',\n",
      "       'BMIN', 'BAVG', 'S5MX', 'S5MN', 'S5AV', 'B5MX', 'B5MN', 'B5AV', 'S25X',\n",
      "       'S25N', 'S25AV', 'S3MX', 'S3MN', 'S3AV', 'S60X', 'S60N', 'S60AV',\n",
      "       'B5MX.1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check which data columns we have\n",
    "# the names are documented at http://www.mesonet.org/index.php/site/about/daily_summaries\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8311, 63)\n"
     ]
    }
   ],
   "source": [
    "# we need to cleanup the data a bit since we asked for all data from both norman stations\n",
    "# and one of them is only ever active at once\n",
    "# let's remove all the rows where there is a -999 in the TMAX variable\n",
    "cleaned_data = data[data['TMAX'] > -900]\n",
    "print(cleaned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's break our data into training, testing, and validation\n",
    "# there are lots of ways to do this and we are picking the fastest for the purposes of time\n",
    "# just breaking it up by year.\n",
    "testing = cleaned_data[cleaned_data['YEAR'] == 2019]\n",
    "validation = cleaned_data[cleaned_data['YEAR'] == 2018]\n",
    "training = cleaned_data[cleaned_data['YEAR'] < 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating multiple different labels to play with for training\n",
    "def get_warmer_than_constant_labels(my_data, target_max_temp):\n",
    "    \"\"\"\n",
    "    Return 1 if the max temperature is greater than the specified max temp and 0 otherwise\n",
    "    \"\"\"\n",
    "    num_labels = my_data.shape[0]\n",
    "    my_labels = np.zeros((num_labels,))\n",
    "\n",
    "    # loop through each day and compare to the specified target\n",
    "    example = 0\n",
    "    for ind in my_data.index:\n",
    "        today_max = float(my_data['TMAX'][ind])\n",
    "\n",
    "        if (today_max >= target_max_temp):\n",
    "            my_labels[example] = 1\n",
    "        example +=1 \n",
    "    return my_labels\n",
    "\n",
    "def get_warmer_than_comparison(my_data):\n",
    "    \"\"\"\n",
    "    Return 1 if the max temperature today is greater than the max temp yesterday\n",
    "    \"\"\"\n",
    "    num_labels = my_data.shape[0]\n",
    "    my_labels = np.zeros((num_labels,))\n",
    "\n",
    "    # now loop through each day and see if the max is higher than the day before\n",
    "    yesterday_max = -10000\n",
    "    example = 0\n",
    "    for ind in my_data.index:\n",
    "        today_max = float(my_data['TMAX'][ind])\n",
    "\n",
    "        if (today_max >= yesterday_max):\n",
    "            my_labels[example] = 1\n",
    "        example +=1 \n",
    "        yesterday_max = today_max\n",
    "    return my_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compute the labels for the training, validation, and testing data\n",
    "# two different approaches, just uncomment the one we want to use\n",
    "# training_labels = get_warmer_than_constant_labels(training, 70)\n",
    "# testing_labels = get_warmer_than_constant_labels(testing, 70)\n",
    "# validation_labels = get_warmer_than_constant_labels(validation, 70)\n",
    "\n",
    "training_labels = get_warmer_than_comparison(training)\n",
    "testing_labels = get_warmer_than_comparison(testing)\n",
    "validation_labels = get_warmer_than_comparison(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we just need to setup our tree\n",
    "tree = DecisionTreeClassifier(max_depth=10)\n",
    "columns = ['TMIN', 'TAVG', 'DMAX', 'DMIN', 'DAVG', 'HMAX', 'HMIN', 'HAVG', 'VDEF', '9AVG', \n",
    "           'HDEG', 'CDEG', 'HTMX', 'WCMN', 'PDIR', 'PDFQ', 'SDIR', 'SDFQ', 'WSMX', 'WSMN', \n",
    "           'WSPD', 'WDEV','WMAX', '2MAX', '2MIN', '2AVG', '2DEV', 'PMAX', 'PMIN', 'PAVG', 'MSLP',\n",
    "           'AMAX', 'ATOT', 'RAIN', 'RNUM', 'RMAX', 'SMAX', 'SMIN', 'SAVG', 'BMAX','BMIN', \n",
    "           'BAVG', 'S5MX', 'S5MN', 'S5AV', 'B5MX', 'B5MN', 'B5AV', 'S25X','S25N', 'S25AV', \n",
    "           'S3MX', 'S3MN', 'S3AV', 'S60X', 'S60N', 'S60AV']\n",
    "tree.fit(training[columns].values, training_labels)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.664548\n",
      "Accuracy 0.674931\n"
     ]
    }
   ],
   "source": [
    "# let's see how well it did\n",
    "predictions = tree.predict(testing[columns].values)\n",
    "auc_score = roc_auc_score(testing_labels, predictions)\n",
    "\n",
    "print(\"AUC: %f\" % auc_score)\n",
    "acc = accuracy_score(testing_labels, predictions)\n",
    "print(\"Accuracy %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can we improve using a forest?\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(training[columns].values, training_labels)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(n_estimators=100)\n",
    "gbrt.fit(training[columns].values, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUC: 0.761194\n",
      "RF Accuracy 0.771350\n",
      "GBRT AUC: 0.778515\n",
      "GBRT Accuracy 0.787879\n"
     ]
    }
   ],
   "source": [
    "# let's see how well they did\n",
    "rf_predictions = rf.predict(testing[columns].values)\n",
    "gbrt_predictions = gbrt.predict(testing[columns].values)\n",
    "\n",
    "auc_score = roc_auc_score(testing_labels, rf_predictions)\n",
    "print(\"RF AUC: %f\" % auc_score)\n",
    "acc = accuracy_score(testing_labels, rf_predictions)\n",
    "print(\"RF Accuracy %f\" % acc)\n",
    "\n",
    "auc_score = roc_auc_score(testing_labels, gbrt_predictions)\n",
    "print(\"GBRT AUC: %f\" % auc_score)\n",
    "acc = accuracy_score(testing_labels, gbrt_predictions)\n",
    "print(\"GBRT Accuracy %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a real-valued label\n",
    "def get_temp_difference(my_data):\n",
    "    \"\"\"\n",
    "    Return the difference in temperature for the day (max - min)\n",
    "    \"\"\"\n",
    "    num_labels = my_data.shape[0]\n",
    "    my_labels = np.zeros((num_labels,))\n",
    "\n",
    "    # now loop through each day and compute the difference\n",
    "    example = 0\n",
    "    for ind in my_data.index:\n",
    "        today_max = float(my_data['TMAX'][ind])\n",
    "        today_min = float(my_data['TMIN'][ind])\n",
    "\n",
    "        my_labels[example] = today_max - today_min\n",
    "        example +=1 \n",
    "    return my_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's setup labels for a real-valued prediction task\n",
    "regression_training_labels = get_temp_difference(training)\n",
    "regression_testing_labels = get_temp_difference(testing)\n",
    "regression_validation_labels = get_temp_difference(validation)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=10, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and create the real-valued tree\n",
    "regression_tree = DecisionTreeRegressor(max_depth=10)\n",
    "regression_cols = ['DMAX', 'DMIN', 'DAVG', 'HMAX', 'HMIN', 'HAVG', 'VDEF', '9AVG', \n",
    "           'HDEG', 'CDEG', 'HTMX', 'WCMN', 'PDIR', 'PDFQ', 'SDIR', 'SDFQ', 'WSMX', 'WSMN', \n",
    "           'WSPD', 'WDEV','WMAX', '2MAX', '2MIN', '2AVG', '2DEV', 'PMAX', 'PMIN', 'PAVG', 'MSLP',\n",
    "           'AMAX', 'ATOT', 'RAIN', 'RNUM', 'RMAX', 'SMAX', 'SMIN', 'SAVG', 'BMAX','BMIN', \n",
    "           'BAVG', 'S5MX', 'S5MN', 'S5AV', 'B5MX', 'B5MN', 'B5AV', 'S25X','S25N', 'S25AV', \n",
    "           'S3MX', 'S3MN', 'S3AV', 'S60X', 'S60N', 'S60AV']\n",
    "regression_tree.fit(training[regression_cols].values, regression_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 29.349370\n",
      "MAE 3.920198\n"
     ]
    }
   ],
   "source": [
    "# let's see how well it did\n",
    "predictions = regression_tree.predict(testing[regression_cols].values)\n",
    "mse = mean_squared_error(regression_testing_labels, predictions)\n",
    "print(\"MSE: %f\" % mse)\n",
    "mae = mean_absolute_error(regression_testing_labels, predictions)\n",
    "print(\"MAE %f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and try real-valued forests for comparison\n",
    "regression_rf = RandomForestRegressor(n_estimators=100)\n",
    "regression_rf.fit(training[regression_cols].values, regression_training_labels)\n",
    "\n",
    "regression_gbrt = GradientBoostingRegressor(n_estimators=100)\n",
    "regression_gbrt.fit(training[regression_cols].values, regression_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF MSE: 14.408144\n",
      "RF MAE 2.668732\n",
      "GBRT MSE: 13.115370\n",
      "GBRT MAE 2.668732\n"
     ]
    }
   ],
   "source": [
    "# let's see how well they did\n",
    "regression_rf_predictions = regression_rf.predict(testing[regression_cols].values)\n",
    "regression_gbrt_predictions = regression_gbrt.predict(testing[regression_cols].values)\n",
    "\n",
    "mse = mean_squared_error(regression_testing_labels, regression_rf_predictions)\n",
    "print(\"RF MSE: %f\" % mse)\n",
    "mae = mean_absolute_error(regression_testing_labels, regression_gbrt_predictions)\n",
    "print(\"RF MAE %f\" % mae)\n",
    "mse = mean_squared_error(regression_testing_labels, regression_gbrt_predictions)\n",
    "print(\"GBRT MSE: %f\" % mse)\n",
    "mae = mean_absolute_error(regression_testing_labels, regression_gbrt_predictions)\n",
    "print(\"GBRT MAE %f\" % mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ou-workshop-2020]",
   "language": "python",
   "name": "conda-env-ou-workshop-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
